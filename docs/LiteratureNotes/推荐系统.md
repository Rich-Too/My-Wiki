KERL: A Knowledge-Guided Reinforcement Learning Model for Sequential Recommendation
>Wang P, Fan Y, Xia L, et al. KERL: A knowledge-guided reinforcement learning model for sequential recommendation[C]//Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 2020: 209-218.

这篇论文除了考虑利用图谱信息推荐与已推荐 item 类似的 item，还考虑了用户的偏好转变和推荐策略的探索性。状态表示由三个部分 concat 而来：gru 嵌入的历史 item、图谱嵌入（就是把 item 对应到图谱上的节点进行 TransE 嵌入，然后做了一个平均）、未来偏好预测（一个 mlp，输入为图谱嵌入）。奖励函数根据模型给出的推荐序列和用户的实际序列比较获得，分为两部分：图谱层面的奖励：对于真实序列和模型推荐序列，将每个时间节点推荐的 item 在图谱上的嵌入做一个 average pooling，比较这两个序列图谱嵌入的相似度；序列层面的奖励：用机器翻译的 BLEU 指标衡量两个序列相似度。感觉图谱奖励可以作为多论问诊奖励部分的一个参考。


- 融合 KG 信息以提高推荐性能
- 三个新颖的扩展，包括状态表示、奖励函数和学习策略

奖励函数：
- 图谱层面的奖励：对于实序列和模型推荐序列：将每个时间节点推荐的 item 在图谱上的 TransE嵌入做一个 average pooling，比较这两个序列图谱嵌入的相似度。
- 序列层面的奖励：用机器翻译的 BLEU 指标衡量两个序列相似度。


Pseudo Dyna-Q: A Reinforcement Learning Framework for Interactive Recommendation


Jin Huang, Wayne Xin Zhao, Hong-Jian Dou, Ji-Rong Wen, and Edward Y. Chang. 2018. Improving Sequential Recommendation with Knowledge-Enhanced Memory Networks. In SIGIR. 505–514.

---
Interactive Recommender System via Knowledge Graph-enhanced Reinforcement Learning 
>Zhou S, Dai X, Chen H, et al. Interactive recommender system via knowledge graph-enhanced reinforcement learning[C]//Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 2020: 179-188.

在 Feed 流式推荐场景中，推荐系统与用户之间的交互性质是一个持续一段时间的多步骤交互过程。在每个 timestep t ，根据对过去交互的观察，推荐代理向用户交付一个 item，并接收来自用户的反馈（例如，单击、购买或跳过）。此过程将一直持续，直到用户离开推荐系统。

这篇文章的目标是为用户不断地推荐 item，构建了一个知识增强的强化学习方案，先用 gcn 对知识图谱做一个节点嵌入，获取每一个 item 的节点表示，然后把为某个用户推荐的历史 item 序列输入一个 gru 网络再做一次嵌入，输出作为 dqn 网络的状态表示，这样 dqn 的状态空间就已经包含了一部分图谱的先验知识。dqn 的动作空间设置为从已推荐 item 出发的多跳节点（文章设置的是两跳包括一跳）。
