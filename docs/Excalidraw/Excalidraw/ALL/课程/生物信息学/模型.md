---

excalidraw-plugin: raw
tags: [excalidraw]

---
==⚠  Switch to EXCALIDRAW VIEW in the MORE OPTIONS menu of this document. ⚠==


# Text Elements
展示思路 ^zAXaMhtA

以一段具体的文本分析来引入nlp的主要任务 ^CBfffQZY

研究目的和背景 ^K1O4X9yd

传统方法 ^tdBWY4kZ

现代方法 ^5tuTD9Kn

概览 ^Yzi6lk3J

BERT模型 ^OylNHBKJ

精选论文，挑选典型模型，两三个就够时间了 ^137MIVgE

总览一下所有的DL模型 ^4I4mU5sY

or ^KaZSgVZx

分研究领域分别报个菜名 ^rAbdbWUj

研究缺陷和前景展望 ^6YBecCmN

CNN、RNN ^uyKnHzFM

RNN（循环神经网络） ^IbqV0Jz1


https://zybuluo.com/hanbingtao/note/541458
https://zhuanlan.zhihu.com/p/30844905 ^I4siK7Ib

全连接神经网络只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的 ^NhSoEjP7

将我 吃 苹果 
三个单词标注词性但是很明显，一个句子中，前一个单词其实对于当前单词的词性预测是有很大影响的，比如预测苹果的时候，由于前面的吃是一个动词，那么很显然苹果作为名词的概率就会远大于动词的概率，因为动词后面接名词很常见，而动词后面接动词很少见。 ^de1Bz1ou

向量表示词的方法有很多，常用的比如one-hot、词袋模型、词嵌入等。
将单词转化为向量最简单的方法是 用 one-hot 编码表示每个词语，one-hot 编码就是将
每个离散取值映射到一个唯一的整数标识，并将该标识转换为一个二元组（0/1 向量）存
在维度过高矩阵稀疏、正交性的问题。而 word embedding（词嵌入），是一种维位、稠密、能够表达
不同词语语义远近的一种过程更适合用用于NLP任务 ，词嵌入最常用的方法是 word2vec
 ^1ryFNt5W

LSTM（Long Short-Term Memory）是一种递归神经网络（Recurrent Neural Network, RNN）的变种，它是由Hochreiter和Schmidhuber在1997年提出的。与标准 RNN 不同，LSTM 具有长期记忆（long-term memory）和短期记忆（short-term memory）的机制，能够更好地处理长序列数据，避免了标准 RNN 中梯度消失和梯度爆炸的问题，使得其在自然语言处理、语音识别、图像处理等领域中有广泛应用。 ^uoxIjM96

注意力机制 ^8SgNwor3

传统seq2seq模型中的编码器会将序列中所有时刻的序列数据转换成一个固定长度的语义变量；然后对于不同时刻的输出解码器共用这个语义变量，注意力机制的目的是为了克服 encoder 对任意句子只能给出一个固定size的表征，而这个表征在遇到长句时则显得包含信息量不够。 ^80muzRDr

引入注意力机制的seq2seq模型会在根据解码器当前的位置对编码器时刻的输出编码器和解码器之间生成多个语义变量（每一时刻会对应一个语义变量）；这样一来每一时刻对应的语义变量能够更加有针对性的保存当前时刻的语义信息；语义变量的数量会随着序列数据长度增加而增加，这就避免了较长的序列数据所产生的数据丢失问题 ^em55uLwQ

传统的seq2seq模型，本质上是一个encoder-decoder结构，编码器和解码器
两部分内部都使用RNN网络，编码器和解码器中间通过语义变量相连接 ^qA0xqEHd

Transformer ^ThtzshSV

在编码部分，每一个的小编码器的输入是前一个小编码器的输出，而每一个小解码器的输
入不光是它的前一个解码器的输出，还包括了整个编码部分的输出 ^alwq7pHE

transformer中使用了6个encoder，为了解决梯度消失的问题，在Encoders和Decoder中
都是用了残差神经网络的结构，即每一个前馈神经网络的输入不光包含上述self-attention
的输出Z，还包含最原始的输入。 ^vnJOo2N5

简单的理解就是把embedding映射到一个更适合做attention的空间，增强表示能力。
把输入词向量用不同的W分成query(Q)，key(K)和value(K)三个部分。Q和K相乘可以看作一个retrieval的过程，Q是查询要求，K是候选结果的title，V是候选结果的具体内容。每个单词的Q和所有其他单词的K做匹配，看和各个单词的相关程度（也就是softmax得到的概率），这个相关程度去和每个单词的V做weighted sum，从各个单词中根据需要提取信息。
Q就是决定了谁来翻动这本字典，用K去找到要查的东西在第几页（即用多少的权重参与进来），V就是我查到的东西是什么 ^zlHrbPKu

如生成machine”时，"机","器","学",""习"的贡献是相同的原句子中任意单词对生成某个目标单词来说影响力都是相同的，这就是模型没有体现出注意力的缘由。我们希望在模型翻译"machine"的时候，"机"，"器"两个字的贡献(权重)更大，当在翻译成"learning"时，"学"，"习"两个字贡献(权重)更大。 ^mx5lAthI

BERT ^y64YaGX7

基于变换器的双向编码器表示技术（Bidirectional Encoder Representations from Transformers，BERT）是用于自然语言处理（NLP）的预训练技术，由Google2018年提出。 ^UUjaq0df

2017 年：Vaswani 等人提出 Transformer 模型，应用于机器翻译任务，显著改善了传统编码器-解码器模型的性能，成为了机器翻译领域的 state-of-the-art 方法。
2018 年：BERT 模型（Bidirectional Encoder Representations from Transformers）被提出，它是一个用于预训练的 Transformer 模型，采用双向 Transformer 编码器来学习上下文相关的词表示，进一步提高了在 NLP 任务中预训练模型的性能。
2019 年：XLNet 模型提出，它采用了一个自回归的方法来预训练 Transformer 模型，允许对所有输入序列的所有排列进行建模，避免了 BERT 模型中的顺序限制。
2019 年：GPT-2 模型（Generative Pre-trained Transformer 2）由 OpenAI 发布，是一个使用未标记数据进行预训练的、具有数亿个参数的 Transformer 模型。GPT-2 在多个 NLP 任务上取得了非常好的结果，并引起了广泛的关注和讨论。
2019 年：T5 模型（Text-to-Text Transfer Transformer）被提出，通过将所有 NLP 任务都视为文本到文本转换问题，让 Transformer 模型能够在多种 NLP 任务中表现出色。
2020 年：GPT-3 模型（Generative Pre-trained Transformer 3）由 OpenAI 发布，是一个使用未标记数据进行预训练的、拥有超过1.5万亿个参数的 Transformer 模型。GPT-3 在多个 NLP 任务上取得了令人惊讶的结果，并引发了社区对大规模深度学习模型的重大探索。 ^5XVOoP9O

http://jalammar.github.io/illustrated-transformer/ ^htO8YcRX

对于QKV矩阵 ^fKYfXn66

"state-of-the-art" 在某个领域或方向上的表现通常是指当前最好、最先进的技术或成果， ^vyXvAyaw

编码器在结构上都是相同的(但它们不共享权重),
完全相同的前馈网络被独立地应用到每个位置。 ^vPq6WVzp

BERT整体框架包含pre-train和fine-tune两个阶段。pre-train阶段模型是在无标注的标签数据上进行训练，fine-tune阶段，BERT模型首先是被pre-train模型参数初始化，然后所有的参数会用下游的有标注的数据进行训练。 ^5oABOi1o

1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。 ^gLKSpr9r

BERT利用MLM进行预训练并且采用深层的双向Transformer组件（单向的Transformer一般被称为Transformer decoder，
其每一个token（符号）只会attend到目前往左的token。而双向的Transformer则被称为Transformer encoder，其每一个
token会attend到所有的token。因此最终生成能融合左右上下文信息的深层双向语言表征。 ^mkNN9JMe

什么是BERT？ ^lReCxdEo

BERT的输入 ^wOjQVfo7

Token 通常指的是文本处理中的一个基本单元。它是由一个或多个连续字符组成的序列，可以是单词、数字、标点符号等。
输入的每个token对应着一个向量表示，这个向量就是该token的表征。 ^ruUBXLJp

[CLS]表示该特征用于分类模型，其对应的最后一个Transformer层输出
被用来聚集整个序列的表征信息。对非分类模型，该符号可以省去。
[SEP]表示分句符号，用于断开输入语料中的两个句子。 ^pX0bCUgY

飞桨的介绍 ^r6yKRdug

C为分类token（[CLS]）对应最后一个Transformer的输出，
 T1i则代表其他token对应最后一个Transformer的输出。对于一些token级别的任务（如，序列标注和问答任务），就把
 Ti输入到额外的输出层中进行预测。对于一些句子级别的任务C（如自然语言推断和情感分类任务），就把C输入到额外的输出层中，这里也就解释了为什么要在每一个token序列前都要插入特定的分类token。 ^HzBfaLf8

Next Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文 ^jfTPFVWW

MLM是指在训练的时候随即从输入语料上mask掉一些单词，然后通过的上下
文预测该单词，该任务非常像我们在中学时期经常做的完形填空。 ^loX2AJUq

BERT的输出 ^wIcXchf3

%%***>>>text element-link:[[展示]]<<<***%%展示讲稿 ^nf5kHW9R


# Embedded files
e816c45d1b75de0df8e52a36e60df81983699bef: [[Pasted Image 20230610170553_218.png]]
734afd7c75546e441424b1336eda2bb14e061279: [[Pasted Image 20230610170608_243.png]]
3fdafb78e61b4c1b3b5a1425868e605f018d9b09: [[Pasted Image 20230610170623_267.png]]
fbb69aa84ee9aee7803e69256056446fb064e9c6: [[Pasted Image 20230610175504_221.png]]
d7f7bc71485d0bfd5c7d74547ca8a44fe78cffbb: [[Pasted Image 20230610181318_218.png]]
f04af094309516d95ab06553673368489212ec7b: [[Pasted Image 20230610181453_216.png]]
ac8ddb9e4181ccca9f0f34a4a1d3b38d0b887bdf: [[Pasted Image 20230610181540_214.png]]
b821752f17fa55fbc0bdb65204249a56ff27a17e: [[Pasted Image 20230610205312_318.png]]
63d0593c5304e0bc3c66111e0187962e3d948c2c: [[Pasted Image 20230610205413_326.png]]
e974c49f1f4635c99e6c5766d2df3c3d48d5522d: [[Pasted Image 20230610214649_314.png]]
464683fceb24c6e5f7ef4b35fe3a739f073e4139: [[Pasted Image 20230610215929_322.png]]

%%
# Drawing
```json
{
	"type": "excalidraw",
	"version": 2,
	"source": "https://github.com/zsviczian/obsidian-excalidraw-plugin/releases/tag/1.9.3",
	"elements": [
		{
			"type": "text",
			"version": 341,
			"versionNonce": 210477452,
			"isDeleted": false,
			"id": "zAXaMhtA",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2064.320484732574,
			"y": -217.8893942910821,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 204.199951171875,
			"height": 61.26850925104581,
			"seed": 1003241805,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": "",
			"locked": false,
			"fontSize": 51.05709104253818,
			"fontFamily": 4,
			"text": "展示思路",
			"rawText": "展示思路",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "展示思路",
			"lineHeight": 1.2,
			"baseline": 50
		},
		{
			"type": "rectangle",
			"version": 343,
			"versionNonce": 211946804,
			"isDeleted": false,
			"id": "iSUy3ihapH9vJuIsHYHvz",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1820.7141554560073,
			"y": -124.46114157309347,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 179,
			"height": 69,
			"seed": 640090883,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "CBfffQZY"
				},
				{
					"id": "vmCB_Vk0pzMWF7aHr8oIi",
					"type": "arrow"
				},
				{
					"id": "7VVhJMtx9WfJET_b9bUfv",
					"type": "arrow"
				},
				{
					"id": "GJTl0l8nIkr5UVysKVZGE",
					"type": "arrow"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 455,
			"versionNonce": 272076812,
			"isDeleted": false,
			"id": "CBfffQZY",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1811.2141554560073,
			"y": -109.16114157309347,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 160,
			"height": 38.4,
			"seed": 125389539,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "以一段具体的文本分析\n来引入nlp的主要任务",
			"rawText": "以一段具体的文本分析来引入nlp的主要任务",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "iSUy3ihapH9vJuIsHYHvz",
			"originalText": "以一段具体的文本分析来引入nlp的主要任务",
			"lineHeight": 1.2,
			"baseline": 35
		},
		{
			"type": "rectangle",
			"version": 282,
			"versionNonce": 1557284532,
			"isDeleted": false,
			"id": "8rFOq1Ye5NqEMDdRA5Y9b",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2089.585822970832,
			"y": -127.95622830171396,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 179,
			"height": 67,
			"seed": 1911323971,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "K1O4X9yd"
				},
				{
					"id": "vmCB_Vk0pzMWF7aHr8oIi",
					"type": "arrow"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 454,
			"versionNonce": 1164029580,
			"isDeleted": false,
			"id": "K1O4X9yd",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2056.085822970832,
			"y": -104.05622830171396,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 112,
			"height": 19.2,
			"seed": 848420067,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "研究目的和背景",
			"rawText": "研究目的和背景",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "8rFOq1Ye5NqEMDdRA5Y9b",
			"originalText": "研究目的和背景",
			"lineHeight": 1.2,
			"baseline": 15
		},
		{
			"type": "arrow",
			"version": 412,
			"versionNonce": 2109689908,
			"isDeleted": false,
			"id": "vmCB_Vk0pzMWF7aHr8oIi",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1902.5858229708322,
			"y": -92.82619059602548,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 73.87166751482482,
			"height": 1.235011317243547,
			"seed": 712940621,
			"groupIds": [],
			"roundness": {
				"type": 2
			},
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"startBinding": {
				"elementId": "8rFOq1Ye5NqEMDdRA5Y9b",
				"gap": 8,
				"focus": 0
			},
			"endBinding": {
				"elementId": "iSUy3ihapH9vJuIsHYHvz",
				"gap": 8,
				"focus": 0
			},
			"lastCommittedPoint": null,
			"startArrowhead": null,
			"endArrowhead": "arrow",
			"points": [
				[
					0,
					0
				],
				[
					73.87166751482482,
					1.235011317243547
				]
			]
		},
		{
			"type": "rectangle",
			"version": 492,
			"versionNonce": 1019477260,
			"isDeleted": false,
			"id": "JHQpDAkquzronmKKHiGZk",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1527.2731421092028,
			"y": -194.1483694876687,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 179,
			"height": 67,
			"seed": 596302157,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "tdBWY4kZ"
				},
				{
					"id": "7VVhJMtx9WfJET_b9bUfv",
					"type": "arrow"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 249,
			"versionNonce": 1599023540,
			"isDeleted": false,
			"id": "tdBWY4kZ",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1469.7731421092028,
			"y": -170.2483694876687,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 64,
			"height": 19.2,
			"seed": 1614277741,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "传统方法",
			"rawText": "传统方法",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "JHQpDAkquzronmKKHiGZk",
			"originalText": "传统方法",
			"lineHeight": 1.2,
			"baseline": 15
		},
		{
			"type": "rectangle",
			"version": 494,
			"versionNonce": 1608022924,
			"isDeleted": false,
			"id": "NETRSjwerpcwwGpU1UgBU",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1532.0392194713047,
			"y": -66.78192145735508,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 179,
			"height": 67,
			"seed": 478461443,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "5tuTD9Kn"
				},
				{
					"id": "GJTl0l8nIkr5UVysKVZGE",
					"type": "arrow"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 293,
			"versionNonce": 78107444,
			"isDeleted": false,
			"id": "5tuTD9Kn",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1474.5392194713047,
			"y": -42.881921457355084,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 64,
			"height": 19.2,
			"seed": 1773164461,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "现代方法",
			"rawText": "现代方法",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "NETRSjwerpcwwGpU1UgBU",
			"originalText": "现代方法",
			"lineHeight": 1.2,
			"baseline": 15
		},
		{
			"type": "arrow",
			"version": 262,
			"versionNonce": 1490627084,
			"isDeleted": false,
			"id": "7VVhJMtx9WfJET_b9bUfv",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1633.7141554560073,
			"y": -113.44799040538584,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 98.44101334680454,
			"height": 23.713530249990498,
			"seed": 933713347,
			"groupIds": [],
			"roundness": {
				"type": 2
			},
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"startBinding": {
				"elementId": "iSUy3ihapH9vJuIsHYHvz",
				"gap": 8,
				"focus": 0
			},
			"endBinding": {
				"elementId": "JHQpDAkquzronmKKHiGZk",
				"gap": 8,
				"focus": 0
			},
			"lastCommittedPoint": null,
			"startArrowhead": null,
			"endArrowhead": "arrow",
			"points": [
				[
					0,
					0
				],
				[
					98.44101334680454,
					-23.713530249990498
				]
			]
		},
		{
			"type": "arrow",
			"version": 272,
			"versionNonce": 472861876,
			"isDeleted": false,
			"id": "GJTl0l8nIkr5UVysKVZGE",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1633.7141554560073,
			"y": -70.81772705241843,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 93.67493598470264,
			"height": 18.392391074388314,
			"seed": 1505077965,
			"groupIds": [],
			"roundness": {
				"type": 2
			},
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"startBinding": {
				"elementId": "iSUy3ihapH9vJuIsHYHvz",
				"gap": 8,
				"focus": 0
			},
			"endBinding": {
				"elementId": "NETRSjwerpcwwGpU1UgBU",
				"gap": 8,
				"focus": 0
			},
			"lastCommittedPoint": null,
			"startArrowhead": null,
			"endArrowhead": "arrow",
			"points": [
				[
					0,
					0
				],
				[
					93.67493598470264,
					18.392391074388314
				]
			]
		},
		{
			"type": "text",
			"version": 298,
			"versionNonce": 2059405452,
			"isDeleted": false,
			"id": "Yzi6lk3J",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1540.8967966669773,
			"y": -250.25980189583078,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 49.8599853515625,
			"height": 29.918207128350854,
			"seed": 16722051,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 24.931839273625712,
			"fontFamily": 4,
			"text": "概览",
			"rawText": "概览",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "概览",
			"lineHeight": 1.2,
			"baseline": 24
		},
		{
			"type": "rectangle",
			"version": 410,
			"versionNonce": 395023924,
			"isDeleted": false,
			"id": "tjet_rs331Ta32iL1U_D1",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1052.5883235492404,
			"y": -319.05257421252713,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 119,
			"height": 57,
			"seed": 255713517,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "OylNHBKJ"
				},
				{
					"id": "Fo4Izquq3HVOekK_jPLyz",
					"type": "arrow"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 395,
			"versionNonce": 692524812,
			"isDeleted": false,
			"id": "OylNHBKJ",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1029.0883235492404,
			"y": -300.15257421252716,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 72,
			"height": 19.2,
			"seed": 718015661,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "BERT模型",
			"rawText": "BERT模型",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "tjet_rs331Ta32iL1U_D1",
			"originalText": "BERT模型",
			"lineHeight": 1.2,
			"baseline": 15
		},
		{
			"type": "rectangle",
			"version": 349,
			"versionNonce": 1134735284,
			"isDeleted": false,
			"id": "_sNlx5SVbBLKCZ5i6IJRm",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1239.8226539435468,
			"y": -188.79322788754922,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 229,
			"height": 68,
			"seed": 132943341,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "137MIVgE"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 536,
			"versionNonce": 1088968076,
			"isDeleted": false,
			"id": "137MIVgE",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1229.3226539435468,
			"y": -173.9932278875492,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 208,
			"height": 38.4,
			"seed": 1515598147,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "精选论文，挑选典型模型，两\n三个就够时间了",
			"rawText": "精选论文，挑选典型模型，两三个就够时间了",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "_sNlx5SVbBLKCZ5i6IJRm",
			"originalText": "精选论文，挑选典型模型，两三个就够时间了",
			"lineHeight": 1.2,
			"baseline": 35
		},
		{
			"type": "rectangle",
			"version": 378,
			"versionNonce": 1620744500,
			"isDeleted": false,
			"id": "pOVY8mA2AT3umGPYVpJiQ",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1243.6031196992358,
			"y": -71.68185467689926,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 229,
			"height": 61,
			"seed": 1575356877,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "rAbdbWUj"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 559,
			"versionNonce": 2128462860,
			"isDeleted": false,
			"id": "rAbdbWUj",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1217.1031196992358,
			"y": -50.781854676899265,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 176,
			"height": 19.2,
			"seed": 1655833987,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "分研究领域分别报个菜名",
			"rawText": "分研究领域分别报个菜名",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "pOVY8mA2AT3umGPYVpJiQ",
			"originalText": "分研究领域分别报个菜名",
			"lineHeight": 1.2,
			"baseline": 15
		},
		{
			"type": "rectangle",
			"version": 219,
			"versionNonce": 970543796,
			"isDeleted": false,
			"id": "yvAC2RJDf9r4ShF2zNeLV",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1512.3801715794773,
			"y": 65.29073346903203,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 214,
			"height": 47,
			"seed": 2105645613,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "4I4mU5sY"
				},
				{
					"id": "5eLNnGEBmOJQGC8dGJVNg",
					"type": "arrow"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 302,
			"versionNonce": 1763795596,
			"isDeleted": false,
			"id": "4I4mU5sY",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1487.5793903294773,
			"y": 79.19073346903204,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 164.3984375,
			"height": 19.2,
			"seed": 1221778733,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "总览一下所有的DL模型",
			"rawText": "总览一下所有的DL模型",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "yvAC2RJDf9r4ShF2zNeLV",
			"originalText": "总览一下所有的DL模型",
			"lineHeight": 1.2,
			"baseline": 15
		},
		{
			"type": "arrow",
			"version": 187,
			"versionNonce": 1040505908,
			"isDeleted": false,
			"id": "5eLNnGEBmOJQGC8dGJVNg",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1446.3888001931364,
			"y": -5.430715770424001,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 27.298673908469482,
			"height": 62.721449239456035,
			"seed": 1839925763,
			"groupIds": [],
			"roundness": {
				"type": 2
			},
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"startBinding": null,
			"endBinding": {
				"elementId": "yvAC2RJDf9r4ShF2zNeLV",
				"gap": 8,
				"focus": 0
			},
			"lastCommittedPoint": null,
			"startArrowhead": null,
			"endArrowhead": "arrow",
			"points": [
				[
					0,
					0
				],
				[
					27.298673908469482,
					62.721449239456035
				]
			]
		},
		{
			"type": "text",
			"version": 146,
			"versionNonce": 913125644,
			"isDeleted": false,
			"id": "KaZSgVZx",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1153.681005899376,
			"y": -119.53766988222446,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 34.032012939453125,
			"height": 41.16989311530911,
			"seed": 564368195,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 34.30824426275759,
			"fontFamily": 4,
			"text": "or",
			"rawText": "or",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "or",
			"lineHeight": 1.2,
			"baseline": 33
		},
		{
			"type": "arrow",
			"version": 223,
			"versionNonce": 386086324,
			"isDeleted": false,
			"id": "qDRMRZnom0ruNLx8agS0H",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1025.4692570647005,
			"y": -97.5606482487928,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 196.5865036899179,
			"height": 2.826238660723618,
			"seed": 1127159779,
			"groupIds": [],
			"roundness": {
				"type": 2
			},
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"startBinding": null,
			"endBinding": {
				"elementId": "IO_3xNI9E54YqHepMvRHK",
				"gap": 8,
				"focus": 0
			},
			"lastCommittedPoint": null,
			"startArrowhead": null,
			"endArrowhead": "arrow",
			"points": [
				[
					0,
					0
				],
				[
					196.5865036899179,
					-2.826238660723618
				]
			]
		},
		{
			"type": "arrow",
			"version": 152,
			"versionNonce": 909848460,
			"isDeleted": false,
			"id": "2Xy-Wa7XAT1dxogx4riWa",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1356.970988720767,
			"y": -86.93425127719176,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 99.79648139667461,
			"height": 1.8432245540170271,
			"seed": 346452269,
			"groupIds": [],
			"roundness": {
				"type": 2
			},
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"startBinding": null,
			"endBinding": null,
			"lastCommittedPoint": null,
			"startArrowhead": null,
			"endArrowhead": "arrow",
			"points": [
				[
					0,
					0
				],
				[
					99.79648139667461,
					-1.8432245540170271
				]
			]
		},
		{
			"type": "arrow",
			"version": 195,
			"versionNonce": 1106927412,
			"isDeleted": false,
			"id": "1RN4TMP3DsRAOMxNsZxp-",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1363.133865810444,
			"y": -107.00060341276587,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 101.78994908793129,
			"height": 1.2448380885799537,
			"seed": 157020589,
			"groupIds": [],
			"roundness": {
				"type": 2
			},
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"startBinding": null,
			"endBinding": null,
			"lastCommittedPoint": null,
			"startArrowhead": null,
			"endArrowhead": "arrow",
			"points": [
				[
					0,
					0
				],
				[
					101.78994908793129,
					-1.2448380885799537
				]
			]
		},
		{
			"type": "rectangle",
			"version": 533,
			"versionNonce": 952458764,
			"isDeleted": false,
			"id": "IO_3xNI9E54YqHepMvRHK",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -820.8827533747826,
			"y": -135.28860200311817,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 179,
			"height": 67,
			"seed": 265505027,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "6YBecCmN"
				},
				{
					"id": "qDRMRZnom0ruNLx8agS0H",
					"type": "arrow"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 708,
			"versionNonce": 659577012,
			"isDeleted": false,
			"id": "6YBecCmN",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -803.3827533747826,
			"y": -111.38860200311817,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 144,
			"height": 19.2,
			"seed": 1227436195,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "研究缺陷和前景展望",
			"rawText": "研究缺陷和前景展望",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "IO_3xNI9E54YqHepMvRHK",
			"originalText": "研究缺陷和前景展望",
			"lineHeight": 1.2,
			"baseline": 15
		},
		{
			"type": "rectangle",
			"version": 140,
			"versionNonce": 1996864652,
			"isDeleted": false,
			"id": "hscWO8YDA1ynuzhUnVN1Z",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1269.0678940429013,
			"y": -316.3508392913575,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 116,
			"height": 52,
			"seed": 2076675117,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "uyKnHzFM"
				},
				{
					"id": "Fo4Izquq3HVOekK_jPLyz",
					"type": "arrow"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 172,
			"versionNonce": 850076212,
			"isDeleted": false,
			"id": "uyKnHzFM",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1258.6429901732724,
			"y": -301.145814055907,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 95.15019226074219,
			"height": 21.589949529099027,
			"seed": 294198435,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 17.991624607582523,
			"fontFamily": 4,
			"text": "CNN、RNN",
			"rawText": "CNN、RNN",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "hscWO8YDA1ynuzhUnVN1Z",
			"originalText": "CNN、RNN",
			"lineHeight": 1.2,
			"baseline": 17
		},
		{
			"type": "arrow",
			"version": 221,
			"versionNonce": 2058767116,
			"isDeleted": false,
			"id": "Fo4Izquq3HVOekK_jPLyz",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1145.0678940429013,
			"y": -290.4119207255811,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 84.47957049366096,
			"height": 0.07818383830829134,
			"seed": 1361213677,
			"groupIds": [],
			"roundness": {
				"type": 2
			},
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"startBinding": {
				"elementId": "hscWO8YDA1ynuzhUnVN1Z",
				"gap": 8,
				"focus": 0
			},
			"endBinding": {
				"elementId": "tjet_rs331Ta32iL1U_D1",
				"gap": 8,
				"focus": 0
			},
			"lastCommittedPoint": null,
			"startArrowhead": null,
			"endArrowhead": "arrow",
			"points": [
				[
					0,
					0
				],
				[
					84.47957049366096,
					-0.07818383830829134
				]
			]
		},
		{
			"type": "text",
			"version": 366,
			"versionNonce": 1794161588,
			"isDeleted": false,
			"id": "IbqV0Jz1",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2331.1543627393526,
			"y": 763.965150571649,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 333.8437194824219,
			"height": 39.632960102495865,
			"seed": 1686643895,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 33.02746675207989,
			"fontFamily": 4,
			"text": "RNN（循环神经网络）",
			"rawText": "RNN（循环神经网络）",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "RNN（循环神经网络）",
			"lineHeight": 1.2,
			"baseline": 32
		},
		{
			"type": "text",
			"version": 579,
			"versionNonce": 105974156,
			"isDeleted": false,
			"id": "8SgNwor3",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2357.5953133991893,
			"y": 1220.1988361120802,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 165.09994506835938,
			"height": 39.632960102495865,
			"seed": 482603097,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/",
			"locked": false,
			"fontSize": 33.02746675207989,
			"fontFamily": 4,
			"text": "注意力机制",
			"rawText": "注意力机制",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "注意力机制",
			"lineHeight": 1.2,
			"baseline": 32
		},
		{
			"type": "text",
			"version": 601,
			"versionNonce": 172434740,
			"isDeleted": false,
			"id": "y64YaGX7",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2430.9517366031914,
			"y": 2832.942998800961,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 110,
			"height": 52.8,
			"seed": 595349945,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 44,
			"fontFamily": 4,
			"text": "BERT",
			"rawText": "BERT",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "BERT",
			"lineHeight": 1.2,
			"baseline": 43
		},
		{
			"type": "text",
			"version": 215,
			"versionNonce": 2098198540,
			"isDeleted": false,
			"id": "I4siK7Ib",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1949.5079392958537,
			"y": 690.9034373349584,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 339.546875,
			"height": 57.599999999999994,
			"seed": 1786666297,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": "",
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "\nhttps://zybuluo.com/hanbingtao/note/541458\nhttps://zhuanlan.zhihu.com/p/30844905",
			"rawText": "\nhttps://zybuluo.com/hanbingtao/note/541458\nhttps://zhuanlan.zhihu.com/p/30844905",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "\nhttps://zybuluo.com/hanbingtao/note/541458\nhttps://zhuanlan.zhihu.com/p/30844905",
			"lineHeight": 1.2,
			"baseline": 54
		},
		{
			"type": "rectangle",
			"version": 358,
			"versionNonce": 370433716,
			"isDeleted": false,
			"id": "rmF5Gje9Ur1BE9ADDFnEx",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2211.1032485987735,
			"y": 824.1279072010299,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 237,
			"height": 77,
			"seed": 984498457,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "NhSoEjP7"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 436,
			"versionNonce": 1257051788,
			"isDeleted": false,
			"id": "NhSoEjP7",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2204.6032485987735,
			"y": 833.8279072010299,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 224,
			"height": 57.599999999999994,
			"seed": 1857619481,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "全连接神经网络只能单独的取处\n理一个个的输入，前一个输入和\n后一个输入是完全没有关系的",
			"rawText": "全连接神经网络只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "rmF5Gje9Ur1BE9ADDFnEx",
			"originalText": "全连接神经网络只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的",
			"lineHeight": 1.2,
			"baseline": 54
		},
		{
			"type": "image",
			"version": 259,
			"versionNonce": 1996384308,
			"isDeleted": false,
			"id": "RbjY9A1Qcurd6OT5iwI8m",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1956.6632904144617,
			"y": 765.4243634540596,
			"strokeColor": "transparent",
			"backgroundColor": "transparent",
			"width": 577.6666002402678,
			"height": 188.543959800643,
			"seed": 369283193,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"status": "pending",
			"fileId": "e816c45d1b75de0df8e52a36e60df81983699bef",
			"scale": [
				1,
				1
			]
		},
		{
			"type": "image",
			"version": 387,
			"versionNonce": 1285711116,
			"isDeleted": false,
			"id": "VpX0JzRKtGAc77jLGw_uz",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1951.306677013762,
			"y": 968.9740945327069,
			"strokeColor": "transparent",
			"backgroundColor": "transparent",
			"width": 610.2855631510413,
			"height": 137.73806112783916,
			"seed": 162620249,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"status": "pending",
			"fileId": "734afd7c75546e441424b1336eda2bb14e061279",
			"scale": [
				1,
				1
			]
		},
		{
			"type": "image",
			"version": 282,
			"versionNonce": 283853236,
			"isDeleted": false,
			"id": "fDHHrwA7xTdGgir395qCP",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1377.6442069881025,
			"y": 806.7666067682927,
			"strokeColor": "transparent",
			"backgroundColor": "transparent",
			"width": 218.66656008737345,
			"height": 201.50377775493428,
			"seed": 1004942007,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"status": "pending",
			"fileId": "3fdafb78e61b4c1b3b5a1425868e605f018d9b09",
			"scale": [
				1,
				1
			]
		},
		{
			"type": "rectangle",
			"version": 339,
			"versionNonce": 933880716,
			"isDeleted": false,
			"id": "PSihhQhgVONL2KvoOlrkl",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2244.802901811333,
			"y": 946.7416385861188,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 283,
			"height": 173,
			"seed": 1422637753,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "de1Bz1ou"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 378,
			"versionNonce": 1526786868,
			"isDeleted": false,
			"id": "de1Bz1ou",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2239.302901811333,
			"y": 956.4416385861189,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 272,
			"height": 153.6,
			"seed": 1373508249,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "将我 吃 苹果 \n三个单词标注词性但是很明显，一个句\n子中，前一个单词其实对于当前单词的\n词性预测是有很大影响的，比如预测苹\n果的时候，由于前面的吃是一个动词，\n那么很显然苹果作为名词的概率就会远\n大于动词的概率，因为动词后面接名词\n很常见，而动词后面接动词很少见。",
			"rawText": "将我 吃 苹果 \n三个单词标注词性但是很明显，一个句子中，前一个单词其实对于当前单词的词性预测是有很大影响的，比如预测苹果的时候，由于前面的吃是一个动词，那么很显然苹果作为名词的概率就会远大于动词的概率，因为动词后面接名词很常见，而动词后面接动词很少见。",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "PSihhQhgVONL2KvoOlrkl",
			"originalText": "将我 吃 苹果 \n三个单词标注词性但是很明显，一个句子中，前一个单词其实对于当前单词的词性预测是有很大影响的，比如预测苹果的时候，由于前面的吃是一个动词，那么很显然苹果作为名词的概率就会远大于动词的概率，因为动词后面接名词很常见，而动词后面接动词很少见。",
			"lineHeight": 1.2,
			"baseline": 150
		},
		{
			"type": "text",
			"version": 304,
			"versionNonce": 37589516,
			"isDeleted": false,
			"id": "1ryFNt5W",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2390.899678537439,
			"y": 421.48135661736717,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 679.8675537109375,
			"height": 108.71634424683813,
			"seed": 823207127,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 15.099492256505298,
			"fontFamily": 4,
			"text": "向量表示词的方法有很多，常用的比如one-hot、词袋模型、词嵌入等。\n将单词转化为向量最简单的方法是 用 one-hot 编码表示每个词语，one-hot 编码就是将\n每个离散取值映射到一个唯一的整数标识，并将该标识转换为一个二元组（0/1 向量）存\n在维度过高矩阵稀疏、正交性的问题。而 word embedding（词嵌入），是一种维位、稠密、能够表达\n不同词语语义远近的一种过程更适合用用于NLP任务 ，词嵌入最常用的方法是 word2vec\n",
			"rawText": "向量表示词的方法有很多，常用的比如one-hot、词袋模型、词嵌入等。\n将单词转化为向量最简单的方法是 用 one-hot 编码表示每个词语，one-hot 编码就是将\n每个离散取值映射到一个唯一的整数标识，并将该标识转换为一个二元组（0/1 向量）存\n在维度过高矩阵稀疏、正交性的问题。而 word embedding（词嵌入），是一种维位、稠密、能够表达\n不同词语语义远近的一种过程更适合用用于NLP任务 ，词嵌入最常用的方法是 word2vec\n",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "向量表示词的方法有很多，常用的比如one-hot、词袋模型、词嵌入等。\n将单词转化为向量最简单的方法是 用 one-hot 编码表示每个词语，one-hot 编码就是将\n每个离散取值映射到一个唯一的整数标识，并将该标识转换为一个二元组（0/1 向量）存\n在维度过高矩阵稀疏、正交性的问题。而 word embedding（词嵌入），是一种维位、稠密、能够表达\n不同词语语义远近的一种过程更适合用用于NLP任务 ，词嵌入最常用的方法是 word2vec\n",
			"lineHeight": 1.2,
			"baseline": 105
		},
		{
			"type": "rectangle",
			"version": 818,
			"versionNonce": 2007327924,
			"isDeleted": false,
			"id": "zuYZ0puHDtgSuP_aLJbYt",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1408.5354022240003,
			"y": 1023.6845052843405,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 604,
			"height": 145,
			"seed": 1426699863,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "uoxIjM96"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 981,
			"versionNonce": 1264021644,
			"isDeleted": false,
			"id": "uoxIjM96",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1402.5354022240003,
			"y": 1028.9845052843405,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 592,
			"height": 134.4,
			"seed": 1890109719,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "LSTM（Long Short-Term Memory）是一种递归神经网络（Recurrent Neural \nNetwork, \nRNN）的变种，它是由Hochreiter和Schmidhuber在1997年提出的。与标准 RNN \n不同，LSTM 具有长期记忆（long-term memory）和短期记忆（short-term \nmemory）的机制，能够更好地处理长序列数据，避免了标准 RNN \n中梯度消失和梯度爆炸的问题，使得其在自然语言处理、语音识别、图像处理等领域\n中有广泛应用。",
			"rawText": "LSTM（Long Short-Term Memory）是一种递归神经网络（Recurrent Neural Network, RNN）的变种，它是由Hochreiter和Schmidhuber在1997年提出的。与标准 RNN 不同，LSTM 具有长期记忆（long-term memory）和短期记忆（short-term memory）的机制，能够更好地处理长序列数据，避免了标准 RNN 中梯度消失和梯度爆炸的问题，使得其在自然语言处理、语音识别、图像处理等领域中有广泛应用。",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "zuYZ0puHDtgSuP_aLJbYt",
			"originalText": "LSTM（Long Short-Term Memory）是一种递归神经网络（Recurrent Neural Network, RNN）的变种，它是由Hochreiter和Schmidhuber在1997年提出的。与标准 RNN 不同，LSTM 具有长期记忆（long-term memory）和短期记忆（short-term memory）的机制，能够更好地处理长序列数据，避免了标准 RNN 中梯度消失和梯度爆炸的问题，使得其在自然语言处理、语音识别、图像处理等领域中有广泛应用。",
			"lineHeight": 1.2,
			"baseline": 131
		},
		{
			"type": "rectangle",
			"version": 717,
			"versionNonce": 1283370548,
			"isDeleted": false,
			"id": "g18Uz6782689xb3pyGE56",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2065.483280618025,
			"y": 1180.357352084898,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 351,
			"height": 126,
			"seed": 309678647,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "80muzRDr"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 939,
			"versionNonce": 1118928652,
			"isDeleted": false,
			"id": "80muzRDr",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2057.983280618025,
			"y": 1185.757352084898,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 336,
			"height": 115.19999999999999,
			"seed": 749291031,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "传统seq2seq模型中的编码器会将序列中所有时\n刻的序列数据转换成一个固定长度的语义变量；\n然后对于不同时刻的输出解码器共用这个语义变\n量，注意力机制的目的是为了克服 encoder \n对任意句子只能给出一个固定size的表征，而这\n个表征在遇到长句时则显得包含信息量不够。",
			"rawText": "传统seq2seq模型中的编码器会将序列中所有时刻的序列数据转换成一个固定长度的语义变量；然后对于不同时刻的输出解码器共用这个语义变量，注意力机制的目的是为了克服 encoder 对任意句子只能给出一个固定size的表征，而这个表征在遇到长句时则显得包含信息量不够。",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "g18Uz6782689xb3pyGE56",
			"originalText": "传统seq2seq模型中的编码器会将序列中所有时刻的序列数据转换成一个固定长度的语义变量；然后对于不同时刻的输出解码器共用这个语义变量，注意力机制的目的是为了克服 encoder 对任意句子只能给出一个固定size的表征，而这个表征在遇到长句时则显得包含信息量不够。",
			"lineHeight": 1.2,
			"baseline": 112
		},
		{
			"type": "rectangle",
			"version": 565,
			"versionNonce": 1079166092,
			"isDeleted": false,
			"id": "ejmKirWHK7hsKBHJdMDAG",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2240.5832256863846,
			"y": 1318.9576633370675,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 392,
			"height": 158,
			"seed": 1460080825,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "em55uLwQ"
				}
			],
			"updated": 1686555763990,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 653,
			"versionNonce": 915577396,
			"isDeleted": false,
			"id": "em55uLwQ",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2235.4465069363846,
			"y": 1330.7576633370675,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 381.7265625,
			"height": 134.4,
			"seed": 600352921,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686555763990,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "引入注意力机制的seq2seq模型会在根据解码器当前的\n位置对编码器时刻的输出编码器和解码器之间生成多\n个语义变量（每一时刻会对应一个语义变量）；这样\n一来每一时刻对应的语义变量能够更加有针对性的保\n存当前时刻的语义信息；语义变量的数量会随着序列\n数据长度增加而增加，这就避免了较长的序列数据所\n产生的数据丢失问题",
			"rawText": "引入注意力机制的seq2seq模型会在根据解码器当前的位置对编码器时刻的输出编码器和解码器之间生成多个语义变量（每一时刻会对应一个语义变量）；这样一来每一时刻对应的语义变量能够更加有针对性的保存当前时刻的语义信息；语义变量的数量会随着序列数据长度增加而增加，这就避免了较长的序列数据所产生的数据丢失问题",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "ejmKirWHK7hsKBHJdMDAG",
			"originalText": "引入注意力机制的seq2seq模型会在根据解码器当前的位置对编码器时刻的输出编码器和解码器之间生成多个语义变量（每一时刻会对应一个语义变量）；这样一来每一时刻对应的语义变量能够更加有针对性的保存当前时刻的语义信息；语义变量的数量会随着序列数据长度增加而增加，这就避免了较长的序列数据所产生的数据丢失问题",
			"lineHeight": 1.2,
			"baseline": 131
		},
		{
			"type": "image",
			"version": 473,
			"versionNonce": 363311412,
			"isDeleted": false,
			"id": "43FWCOxVzBVDgQKqtJOXW",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1838.5832256863846,
			"y": 1318.1459437422643,
			"strokeColor": "transparent",
			"backgroundColor": "transparent",
			"width": 474.54109493573196,
			"height": 274.22251797515656,
			"seed": 530283063,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"status": "pending",
			"fileId": "fbb69aa84ee9aee7803e69256056446fb064e9c6",
			"scale": [
				1,
				1
			]
		},
		{
			"type": "text",
			"version": 370,
			"versionNonce": 1053699084,
			"isDeleted": false,
			"id": "qA0xqEHd",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2240.5832256863846,
			"y": 1602.3684617174208,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 539.796875,
			"height": 38.4,
			"seed": 2118004471,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "传统的seq2seq模型，本质上是一个encoder-decoder结构，编码器和解码器\n两部分内部都使用RNN网络，编码器和解码器中间通过语义变量相连接",
			"rawText": "传统的seq2seq模型，本质上是一个encoder-decoder结构，编码器和解码器\n两部分内部都使用RNN网络，编码器和解码器中间通过语义变量相连接",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "传统的seq2seq模型，本质上是一个encoder-decoder结构，编码器和解码器\n两部分内部都使用RNN网络，编码器和解码器中间通过语义变量相连接",
			"lineHeight": 1.2,
			"baseline": 35
		},
		{
			"type": "rectangle",
			"version": 443,
			"versionNonce": 2105764532,
			"isDeleted": false,
			"id": "f1TrL9Uc-ajYl3r4W8Ksh",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1690.7863506863846,
			"y": 1602.3684617174208,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 346,
			"height": 145,
			"seed": 815198327,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "mx5lAthI"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 561,
			"versionNonce": 516213388,
			"isDeleted": false,
			"id": "mx5lAthI",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1685.2355694363846,
			"y": 1607.6684617174208,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 334.8984375,
			"height": 134.4,
			"seed": 1500218007,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399116,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "如生成machine”时，\"机\",\"器\",\"学\",\"\"习\"的贡献\n是相同的原句子中任意单词对生成某个目标单\n词来说影响力都是相同的，这就是模型没有体\n现出注意力的缘由。我们希望在模型翻译\"mach\nine\"的时候，\"机\"，\"器\"两个字的贡献(权重)更大\n，当在翻译成\"learning\"时，\"学\"，\"习\"两个字贡\n献(权重)更大。",
			"rawText": "如生成machine”时，\"机\",\"器\",\"学\",\"\"习\"的贡献是相同的原句子中任意单词对生成某个目标单词来说影响力都是相同的，这就是模型没有体现出注意力的缘由。我们希望在模型翻译\"machine\"的时候，\"机\"，\"器\"两个字的贡献(权重)更大，当在翻译成\"learning\"时，\"学\"，\"习\"两个字贡献(权重)更大。",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "f1TrL9Uc-ajYl3r4W8Ksh",
			"originalText": "如生成machine”时，\"机\",\"器\",\"学\",\"\"习\"的贡献是相同的原句子中任意单词对生成某个目标单词来说影响力都是相同的，这就是模型没有体现出注意力的缘由。我们希望在模型翻译\"machine\"的时候，\"机\"，\"器\"两个字的贡献(权重)更大，当在翻译成\"learning\"时，\"学\"，\"习\"两个字贡献(权重)更大。",
			"lineHeight": 1.2,
			"baseline": 131
		},
		{
			"type": "rectangle",
			"version": 660,
			"versionNonce": 883684404,
			"isDeleted": false,
			"id": "W5Ls8OtTIfhqqW-8ayUOY",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2239.0216796833292,
			"y": 2801.4349211269346,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 673,
			"height": 49,
			"seed": 32930071,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "UUjaq0df"
				}
			],
			"updated": 1686554399116,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 659,
			"versionNonce": 862165260,
			"isDeleted": false,
			"id": "UUjaq0df",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2233.7755859333292,
			"y": 2806.7349211269348,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 662.5078125,
			"height": 38.4,
			"seed": 90351095,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "基于变换器的双向编码器表示技术（Bidirectional Encoder Representations from \nTransformers，BERT）是用于自然语言处理（NLP）的预训练技术，由Google2018年提出。",
			"rawText": "基于变换器的双向编码器表示技术（Bidirectional Encoder Representations from Transformers，BERT）是用于自然语言处理（NLP）的预训练技术，由Google2018年提出。",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "W5Ls8OtTIfhqqW-8ayUOY",
			"originalText": "基于变换器的双向编码器表示技术（Bidirectional Encoder Representations from Transformers，BERT）是用于自然语言处理（NLP）的预训练技术，由Google2018年提出。",
			"lineHeight": 1.2,
			"baseline": 35
		},
		{
			"type": "text",
			"version": 807,
			"versionNonce": 1493650868,
			"isDeleted": false,
			"id": "ThtzshSV",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2363.2239279573373,
			"y": 1824.333898352594,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 192.5574493408203,
			"height": 39.632960102495865,
			"seed": 842197497,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 33.02746675207989,
			"fontFamily": 4,
			"text": "Transformer",
			"rawText": "Transformer",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "Transformer",
			"lineHeight": 1.2,
			"baseline": 32
		},
		{
			"type": "text",
			"version": 718,
			"versionNonce": 405051276,
			"isDeleted": false,
			"id": "alwq7pHE",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2381.1666506317365,
			"y": 2191.1195987344468,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 489.44952392578125,
			"height": 30.129365588355835,
			"seed": 1085373945,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 12.553902328481598,
			"fontFamily": 4,
			"text": "在编码部分，每一个的小编码器的输入是前一个小编码器的输出，而每一个小解码器的输\n入不光是它的前一个解码器的输出，还包括了整个编码部分的输出",
			"rawText": "在编码部分，每一个的小编码器的输入是前一个小编码器的输出，而每一个小解码器的输\n入不光是它的前一个解码器的输出，还包括了整个编码部分的输出",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "在编码部分，每一个的小编码器的输入是前一个小编码器的输出，而每一个小解码器的输\n入不光是它的前一个解码器的输出，还包括了整个编码部分的输出",
			"lineHeight": 1.2,
			"baseline": 27
		},
		{
			"type": "image",
			"version": 731,
			"versionNonce": 1539012404,
			"isDeleted": false,
			"id": "ytyMdRip2_QRSpxIvx4X9",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1793.7960738162797,
			"y": 1992.1346973922373,
			"strokeColor": "transparent",
			"backgroundColor": "transparent",
			"width": 293.2712544225599,
			"height": 188.87492966324115,
			"seed": 1207138263,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"status": "pending",
			"fileId": "d7f7bc71485d0bfd5c7d74547ca8a44fe78cffbb",
			"scale": [
				1,
				1
			]
		},
		{
			"type": "image",
			"version": 770,
			"versionNonce": 1765918220,
			"isDeleted": false,
			"id": "RG8B8-3efMVYgTAoCD8M_",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1487.549295590325,
			"y": 1889.2911208553862,
			"strokeColor": "transparent",
			"backgroundColor": "transparent",
			"width": 353.2829940530805,
			"height": 246.05487122465283,
			"seed": 499510903,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"status": "pending",
			"fileId": "f04af094309516d95ab06553673368489212ec7b",
			"scale": [
				1,
				1
			]
		},
		{
			"type": "image",
			"version": 810,
			"versionNonce": 422473908,
			"isDeleted": false,
			"id": "-pnkRo7AphZqiej3L9_21",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1486.277777491597,
			"y": 2142.883713328829,
			"strokeColor": "transparent",
			"backgroundColor": "transparent",
			"width": 341.56841372250545,
			"height": 174.53008304748656,
			"seed": 594614649,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"status": "pending",
			"fileId": "ac8ddb9e4181ccca9f0f34a4a1d3b38d0b887bdf",
			"scale": [
				1,
				1
			]
		},
		{
			"type": "text",
			"version": 850,
			"versionNonce": 733291660,
			"isDeleted": false,
			"id": "vnJOo2N5",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1799.6486943815469,
			"y": 2259.3085851719534,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 623.2536010742188,
			"height": 57.13258480126423,
			"seed": 506455,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 15.87016244479562,
			"fontFamily": 4,
			"text": "transformer中使用了6个encoder，为了解决梯度消失的问题，在Encoders和Decoder中\n都是用了残差神经网络的结构，即每一个前馈神经网络的输入不光包含上述self-attention\n的输出Z，还包含最原始的输入。",
			"rawText": "transformer中使用了6个encoder，为了解决梯度消失的问题，在Encoders和Decoder中\n都是用了残差神经网络的结构，即每一个前馈神经网络的输入不光包含上述self-attention\n的输出Z，还包含最原始的输入。",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "transformer中使用了6个encoder，为了解决梯度消失的问题，在Encoders和Decoder中\n都是用了残差神经网络的结构，即每一个前馈神经网络的输入不光包含上述self-attention\n的输出Z，还包含最原始的输入。",
			"lineHeight": 1.2,
			"baseline": 54
		},
		{
			"type": "rectangle",
			"version": 1423,
			"versionNonce": 1896233524,
			"isDeleted": false,
			"id": "116CP_En0eR1hyorosNDU",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2481.8075179886782,
			"y": 2350.141918803221,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 737,
			"height": 152,
			"seed": 1624075001,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "zlHrbPKu"
				}
			],
			"updated": 1686554399117,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 1835,
			"versionNonce": 892908300,
			"isDeleted": false,
			"id": "zlHrbPKu",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2476.8075179886782,
			"y": 2358.941918803221,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 719.921875,
			"height": 134.4,
			"seed": 1133673559,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 16,
			"fontFamily": 4,
			"text": "简单的理解就是把embedding映射到一个更适合做attention的空间，增强表示能力。\n把输入词向量用不同的W分成query(Q)，key(K)和value(K)三个部分。Q和K相乘可以看作一个retrieval\n的过程，Q是查询要求，K是候选结果的title，V是候选结果的具体内容。每个单词的Q和所有其他单词\n的K做匹配，看和各个单词的相关程度（也就是softmax得到的概率），这个相关程度去和每个单词的V\n做weighted sum，从各个单词中根据需要提取信息。\nQ就是决定了谁来翻动这本字典，用K去找到要查的东西在第几页（即用多少的权重参与进来），V就是\n我查到的东西是什么",
			"rawText": "简单的理解就是把embedding映射到一个更适合做attention的空间，增强表示能力。\n把输入词向量用不同的W分成query(Q)，key(K)和value(K)三个部分。Q和K相乘可以看作一个retrieval的过程，Q是查询要求，K是候选结果的title，V是候选结果的具体内容。每个单词的Q和所有其他单词的K做匹配，看和各个单词的相关程度（也就是softmax得到的概率），这个相关程度去和每个单词的V做weighted sum，从各个单词中根据需要提取信息。\nQ就是决定了谁来翻动这本字典，用K去找到要查的东西在第几页（即用多少的权重参与进来），V就是我查到的东西是什么",
			"textAlign": "left",
			"verticalAlign": "middle",
			"containerId": "116CP_En0eR1hyorosNDU",
			"originalText": "简单的理解就是把embedding映射到一个更适合做attention的空间，增强表示能力。\n把输入词向量用不同的W分成query(Q)，key(K)和value(K)三个部分。Q和K相乘可以看作一个retrieval的过程，Q是查询要求，K是候选结果的title，V是候选结果的具体内容。每个单词的Q和所有其他单词的K做匹配，看和各个单词的相关程度（也就是softmax得到的概率），这个相关程度去和每个单词的V做weighted sum，从各个单词中根据需要提取信息。\nQ就是决定了谁来翻动这本字典，用K去找到要查的东西在第几页（即用多少的权重参与进来），V就是我查到的东西是什么",
			"lineHeight": 1.2,
			"baseline": 131
		},
		{
			"type": "rectangle",
			"version": 1467,
			"versionNonce": 1579733940,
			"isDeleted": false,
			"id": "w7IEJwsvlaHrUy6CcPpXe",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1111.8445682469837,
			"y": 1941.9852279777892,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 848,
			"height": 334,
			"seed": 560042423,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "5XVOoP9O"
				}
			],
			"updated": 1686554399117,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 1780,
			"versionNonce": 340160908,
			"isDeleted": false,
			"id": "5XVOoP9O",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1106.8445682469837,
			"y": 1946.9852279777892,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 834.8818359375,
			"height": 323.99999999999994,
			"seed": 2139495769,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 18,
			"fontFamily": 4,
			"text": "2017 年：Vaswani 等人提出 Transformer 模型，应用于机器翻译任务，显著改善了传统编码器-\n解码器模型的性能，成为了机器翻译领域的 state-of-the-art 方法。\n2018 年：BERT 模型（Bidirectional Encoder Representations from \nTransformers）被提出，它是一个用于预训练的 Transformer 模型，采用双向 Transformer \n编码器来学习上下文相关的词表示，进一步提高了在 NLP 任务中预训练模型的性能。\n2019 年：XLNet 模型提出，它采用了一个自回归的方法来预训练 Transformer \n模型，允许对所有输入序列的所有排列进行建模，避免了 BERT 模型中的顺序限制。\n2019 年：GPT-2 模型（Generative Pre-trained Transformer 2）由 OpenAI \n发布，是一个使用未标记数据进行预训练的、具有数亿个参数的 Transformer 模型。GPT-2 在多个 NLP \n任务上取得了非常好的结果，并引起了广泛的关注和讨论。\n2019 年：T5 模型（Text-to-Text Transfer Transformer）被提出，通过将所有 NLP \n任务都视为文本到文本转换问题，让 Transformer 模型能够在多种 NLP 任务中表现出色。\n2020 年：GPT-3 模型（Generative Pre-trained Transformer 3）由 OpenAI \n发布，是一个使用未标记数据进行预训练的、拥有超过1.5万亿个参数的 Transformer 模型。GPT-3 \n在多个 NLP 任务上取得了令人惊讶的结果，并引发了社区对大规模深度学习模型的重大探索。",
			"rawText": "2017 年：Vaswani 等人提出 Transformer 模型，应用于机器翻译任务，显著改善了传统编码器-解码器模型的性能，成为了机器翻译领域的 state-of-the-art 方法。\n2018 年：BERT 模型（Bidirectional Encoder Representations from Transformers）被提出，它是一个用于预训练的 Transformer 模型，采用双向 Transformer 编码器来学习上下文相关的词表示，进一步提高了在 NLP 任务中预训练模型的性能。\n2019 年：XLNet 模型提出，它采用了一个自回归的方法来预训练 Transformer 模型，允许对所有输入序列的所有排列进行建模，避免了 BERT 模型中的顺序限制。\n2019 年：GPT-2 模型（Generative Pre-trained Transformer 2）由 OpenAI 发布，是一个使用未标记数据进行预训练的、具有数亿个参数的 Transformer 模型。GPT-2 在多个 NLP 任务上取得了非常好的结果，并引起了广泛的关注和讨论。\n2019 年：T5 模型（Text-to-Text Transfer Transformer）被提出，通过将所有 NLP 任务都视为文本到文本转换问题，让 Transformer 模型能够在多种 NLP 任务中表现出色。\n2020 年：GPT-3 模型（Generative Pre-trained Transformer 3）由 OpenAI 发布，是一个使用未标记数据进行预训练的、拥有超过1.5万亿个参数的 Transformer 模型。GPT-3 在多个 NLP 任务上取得了令人惊讶的结果，并引发了社区对大规模深度学习模型的重大探索。",
			"textAlign": "left",
			"verticalAlign": "middle",
			"containerId": "w7IEJwsvlaHrUy6CcPpXe",
			"originalText": "2017 年：Vaswani 等人提出 Transformer 模型，应用于机器翻译任务，显著改善了传统编码器-解码器模型的性能，成为了机器翻译领域的 state-of-the-art 方法。\n2018 年：BERT 模型（Bidirectional Encoder Representations from Transformers）被提出，它是一个用于预训练的 Transformer 模型，采用双向 Transformer 编码器来学习上下文相关的词表示，进一步提高了在 NLP 任务中预训练模型的性能。\n2019 年：XLNet 模型提出，它采用了一个自回归的方法来预训练 Transformer 模型，允许对所有输入序列的所有排列进行建模，避免了 BERT 模型中的顺序限制。\n2019 年：GPT-2 模型（Generative Pre-trained Transformer 2）由 OpenAI 发布，是一个使用未标记数据进行预训练的、具有数亿个参数的 Transformer 模型。GPT-2 在多个 NLP 任务上取得了非常好的结果，并引起了广泛的关注和讨论。\n2019 年：T5 模型（Text-to-Text Transfer Transformer）被提出，通过将所有 NLP 任务都视为文本到文本转换问题，让 Transformer 模型能够在多种 NLP 任务中表现出色。\n2020 年：GPT-3 模型（Generative Pre-trained Transformer 3）由 OpenAI 发布，是一个使用未标记数据进行预训练的、拥有超过1.5万亿个参数的 Transformer 模型。GPT-3 在多个 NLP 任务上取得了令人惊讶的结果，并引发了社区对大规模深度学习模型的重大探索。",
			"lineHeight": 1.2,
			"baseline": 320
		},
		{
			"type": "image",
			"version": 463,
			"versionNonce": 247217460,
			"isDeleted": false,
			"id": "BL9ttfrBk2M4batXFuBOA",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2374.0601230300913,
			"y": 1886.5905023450966,
			"strokeColor": "transparent",
			"backgroundColor": "transparent",
			"width": 435.0229462804061,
			"height": 283.22922528765355,
			"seed": 2062927224,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"status": "pending",
			"fileId": "b821752f17fa55fbc0bdb65204249a56ff27a17e",
			"scale": [
				1,
				1
			]
		},
		{
			"type": "image",
			"version": 476,
			"versionNonce": 2014863372,
			"isDeleted": false,
			"id": "PwIiyv9DcyEDx7ta3CYNi",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1797.575855704219,
			"y": 1832.3853151188043,
			"strokeColor": "transparent",
			"backgroundColor": "transparent",
			"width": 296.69500764252865,
			"height": 153.96672745085766,
			"seed": 1030645768,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"status": "pending",
			"fileId": "63d0593c5304e0bc3c66111e0187962e3d948c2c",
			"scale": [
				1,
				1
			]
		},
		{
			"type": "arrow",
			"version": 531,
			"versionNonce": 2082895540,
			"isDeleted": false,
			"id": "fXUHyopreBSP3MG5FUDvx",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2098.500736546531,
			"y": 1938.2146958006592,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 319.32243818080406,
			"height": 93.91839531456299,
			"seed": 1900223496,
			"groupIds": [],
			"roundness": {
				"type": 2
			},
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"startBinding": null,
			"endBinding": null,
			"lastCommittedPoint": null,
			"startArrowhead": null,
			"endArrowhead": "arrow",
			"points": [
				[
					0,
					0
				],
				[
					236.36122544357067,
					-93.91839531456299
				],
				[
					319.32243818080406,
					-58.95986470635694
				]
			]
		},
		{
			"type": "rectangle",
			"version": 175,
			"versionNonce": 413695628,
			"isDeleted": false,
			"id": "r-CtpQ2YbW-dtjfVu0fCQ",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1677.7889488921815,
			"y": 1816.6834951945714,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 278,
			"height": 41,
			"seed": 1504130312,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"id": "vPq6WVzp",
					"type": "text"
				}
			],
			"updated": 1686554399117,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 538,
			"versionNonce": 51216436,
			"isDeleted": false,
			"id": "vPq6WVzp",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1672.563469766205,
			"y": 1821.9241901204555,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 267.5490417480469,
			"height": 30.518610148231836,
			"seed": 1433146744,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 12.716087561763265,
			"fontFamily": 4,
			"text": "编码器在结构上都是相同的(但它们不共享权重),\n完全相同的前馈网络被独立地应用到每个位置。",
			"rawText": "编码器在结构上都是相同的(但它们不共享权重),\n完全相同的前馈网络被独立地应用到每个位置。",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "r-CtpQ2YbW-dtjfVu0fCQ",
			"originalText": "编码器在结构上都是相同的(但它们不共享权重),\n完全相同的前馈网络被独立地应用到每个位置。",
			"lineHeight": 1.2,
			"baseline": 27
		},
		{
			"type": "text",
			"version": 169,
			"versionNonce": 514903308,
			"isDeleted": false,
			"id": "htO8YcRX",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2164.66894622406,
			"y": 1787.8845867048594,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 416.8388671875,
			"height": 21.599999999999998,
			"seed": 891800696,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": "http://jalammar.github.io/illustrated-transformer/",
			"locked": false,
			"fontSize": 18,
			"fontFamily": 4,
			"text": "http://jalammar.github.io/illustrated-transformer/",
			"rawText": "http://jalammar.github.io/illustrated-transformer/",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "http://jalammar.github.io/illustrated-transformer/",
			"lineHeight": 1.2,
			"baseline": 17
		},
		{
			"type": "diamond",
			"version": 278,
			"versionNonce": 2069322164,
			"isDeleted": false,
			"id": "U6mo1IKdVGxXtRz_iPn3j",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2636.973818995902,
			"y": 2312.513702805613,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 161,
			"height": 108,
			"seed": 1040840568,
			"groupIds": [],
			"roundness": {
				"type": 2
			},
			"boundElements": [
				{
					"type": "text",
					"id": "fKYfXn66"
				}
			],
			"updated": 1686554399117,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 210,
			"versionNonce": 195775372,
			"isDeleted": false,
			"id": "fKYfXn66",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2586.598818995902,
			"y": 2344.913702805613,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 60.75,
			"height": 43.199999999999996,
			"seed": 1245608712,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 18,
			"fontFamily": 4,
			"text": "对于QK\nV矩阵",
			"rawText": "对于QKV矩阵",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "U6mo1IKdVGxXtRz_iPn3j",
			"originalText": "对于QKV矩阵",
			"lineHeight": 1.2,
			"baseline": 39
		},
		{
			"type": "text",
			"version": 58,
			"versionNonce": 1344930612,
			"isDeleted": false,
			"id": "vyXvAyaw",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2418.819626000635,
			"y": 325.4259168555919,
			"strokeColor": "#e03131",
			"backgroundColor": "transparent",
			"width": 706.25390625,
			"height": 21.599999999999998,
			"seed": 1714330376,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 18,
			"fontFamily": 4,
			"text": "\"state-of-the-art\" 在某个领域或方向上的表现通常是指当前最好、最先进的技术或成果，",
			"rawText": "\"state-of-the-art\" 在某个领域或方向上的表现通常是指当前最好、最先进的技术或成果，",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "\"state-of-the-art\" 在某个领域或方向上的表现通常是指当前最好、最先进的技术或成果，",
			"lineHeight": 1.2,
			"baseline": 17
		},
		{
			"type": "rectangle",
			"version": 1171,
			"versionNonce": 567409164,
			"isDeleted": false,
			"id": "NI5xahwxcwUaJI5288jrN",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2241.558950877431,
			"y": 2865.955005248468,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 772,
			"height": 47,
			"seed": 380746504,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"id": "5oABOi1o",
					"type": "text"
				}
			],
			"updated": 1686554399117,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 1078,
			"versionNonce": 401308852,
			"isDeleted": false,
			"id": "5oABOi1o",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2236.558950877431,
			"y": 2871.3356145406615,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 739.8883056640625,
			"height": 36.23878141561271,
			"seed": 1978210424,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 15.099492256505298,
			"fontFamily": 4,
			"text": "BERT整体框架包含pre-train和fine-tune两个阶段。pre-train阶段模型是在无标注的标签数据上进行训练，fine-\ntune阶段，BERT模型首先是被pre-train模型参数初始化，然后所有的参数会用下游的有标注的数据进行训练。",
			"rawText": "BERT整体框架包含pre-train和fine-tune两个阶段。pre-train阶段模型是在无标注的标签数据上进行训练，fine-tune阶段，BERT模型首先是被pre-train模型参数初始化，然后所有的参数会用下游的有标注的数据进行训练。",
			"textAlign": "left",
			"verticalAlign": "middle",
			"containerId": "NI5xahwxcwUaJI5288jrN",
			"originalText": "BERT整体框架包含pre-train和fine-tune两个阶段。pre-train阶段模型是在无标注的标签数据上进行训练，fine-tune阶段，BERT模型首先是被pre-train模型参数初始化，然后所有的参数会用下游的有标注的数据进行训练。",
			"lineHeight": 1.2,
			"baseline": 32
		},
		{
			"type": "rectangle",
			"version": 175,
			"versionNonce": 1277089932,
			"isDeleted": false,
			"id": "DSj9cNOW6b4CZ0BQGegeb",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2432.4483989190107,
			"y": 2925.759245239425,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 496,
			"height": 83,
			"seed": 1757511544,
			"groupIds": [],
			"roundness": {
				"type": 3
			},
			"boundElements": [
				{
					"type": "text",
					"id": "gLKSpr9r"
				}
			],
			"updated": 1686554399117,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 220,
			"versionNonce": 742508084,
			"isDeleted": false,
			"id": "gLKSpr9r",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2427.4483989190107,
			"y": 2931.0204638238124,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 485.8712463378906,
			"height": 72.47756283122543,
			"seed": 1419038472,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 15.099492256505298,
			"fontFamily": 4,
			"text": "1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言\n表征。2）预训练后，只需要添加一个额外的输出层进行fine-\ntune，就可以在各种各样的下游任务中取得state-of-the-\nart的表现。在这过程中并不需要对BERT进行任务特定的结构修改。",
			"rawText": "1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。",
			"textAlign": "left",
			"verticalAlign": "middle",
			"containerId": "DSj9cNOW6b4CZ0BQGegeb",
			"originalText": "1）采用MLM对双向的Transformers进行预训练，以生成深层的双向语言表征。2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。",
			"lineHeight": 1.2,
			"baseline": 69
		},
		{
			"type": "text",
			"version": 61,
			"versionNonce": 105637644,
			"isDeleted": false,
			"id": "mkNN9JMe",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2424.858593983403,
			"y": 3033.7179727674807,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 820.289794921875,
			"height": 54.35817212341907,
			"seed": 1337819400,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 15.099492256505298,
			"fontFamily": 4,
			"text": "BERT利用MLM进行预训练并且采用深层的双向Transformer组件（单向的Transformer一般被称为Transformer decoder，\n其每一个token（符号）只会attend到目前往左的token。而双向的Transformer则被称为Transformer encoder，其每一个\ntoken会attend到所有的token。因此最终生成能融合左右上下文信息的深层双向语言表征。",
			"rawText": "BERT利用MLM进行预训练并且采用深层的双向Transformer组件（单向的Transformer一般被称为Transformer decoder，\n其每一个token（符号）只会attend到目前往左的token。而双向的Transformer则被称为Transformer encoder，其每一个\ntoken会attend到所有的token。因此最终生成能融合左右上下文信息的深层双向语言表征。",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "BERT利用MLM进行预训练并且采用深层的双向Transformer组件（单向的Transformer一般被称为Transformer decoder，\n其每一个token（符号）只会attend到目前往左的token。而双向的Transformer则被称为Transformer encoder，其每一个\ntoken会attend到所有的token。因此最终生成能融合左右上下文信息的深层双向语言表征。",
			"lineHeight": 1.2,
			"baseline": 50
		},
		{
			"type": "text",
			"version": 30,
			"versionNonce": 1108533172,
			"isDeleted": false,
			"id": "lReCxdEo",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2351.367012385954,
			"y": 2757.1544582927595,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 98.08497619628906,
			"height": 18.119390707806357,
			"seed": 1672761464,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": "https://zhuanlan.zhihu.com/p/98855346",
			"locked": false,
			"fontSize": 15.099492256505298,
			"fontFamily": 4,
			"text": "什么是BERT？",
			"rawText": "什么是BERT？",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "什么是BERT？",
			"lineHeight": 1.2,
			"baseline": 14
		},
		{
			"type": "text",
			"version": 110,
			"versionNonce": 1154688396,
			"isDeleted": false,
			"id": "wOjQVfo7",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2445.9966475984165,
			"y": 3126.4574042153045,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 100.70497131347656,
			"height": 21.977012631538333,
			"seed": 1186176632,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 18.314177192948613,
			"fontFamily": 4,
			"text": "BERT的输入",
			"rawText": "BERT的输入",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "BERT的输入",
			"lineHeight": 1.2,
			"baseline": 18
		},
		{
			"type": "text",
			"version": 224,
			"versionNonce": 25479476,
			"isDeleted": false,
			"id": "wIcXchf3",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1873.225996754035,
			"y": 3118.852839895347,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 100.70497131347656,
			"height": 21.977012631538333,
			"seed": 1511443064,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 18.314177192948613,
			"fontFamily": 4,
			"text": "BERT的输出",
			"rawText": "BERT的输出",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "BERT的输出",
			"lineHeight": 1.2,
			"baseline": 18
		},
		{
			"type": "image",
			"version": 151,
			"versionNonce": 1559139340,
			"isDeleted": false,
			"id": "WnPYOmY5n2Ks1jq1B57k6",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2432.9616418165915,
			"y": 3160.4891070727454,
			"strokeColor": "transparent",
			"backgroundColor": "transparent",
			"width": 532.7120498370083,
			"height": 173.06809859997895,
			"seed": 2090552072,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"status": "pending",
			"fileId": "e974c49f1f4635c99e6c5766d2df3c3d48d5522d",
			"scale": [
				1,
				1
			]
		},
		{
			"type": "text",
			"version": 48,
			"versionNonce": 503153332,
			"isDeleted": false,
			"id": "ruUBXLJp",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2413.15655195592,
			"y": 549.3341382968375,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 816.8195190429688,
			"height": 36.23878141561271,
			"seed": 1726841608,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": null,
			"locked": false,
			"fontSize": 15.099492256505298,
			"fontFamily": 4,
			"text": "Token 通常指的是文本处理中的一个基本单元。它是由一个或多个连续字符组成的序列，可以是单词、数字、标点符号等。\n输入的每个token对应着一个向量表示，这个向量就是该token的表征。",
			"rawText": "Token 通常指的是文本处理中的一个基本单元。它是由一个或多个连续字符组成的序列，可以是单词、数字、标点符号等。\n输入的每个token对应着一个向量表示，这个向量就是该token的表征。",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "Token 通常指的是文本处理中的一个基本单元。它是由一个或多个连续字符组成的序列，可以是单词、数字、标点符号等。\n输入的每个token对应着一个向量表示，这个向量就是该token的表征。",
			"lineHeight": 1.2,
			"baseline": 32
		},
		{
			"type": "text",
			"version": 156,
			"versionNonce": 1378937780,
			"isDeleted": false,
			"id": "pX0bCUgY",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2381.688133850245,
			"y": 3327.294859392376,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 473.8096008300781,
			"height": 54.35817212341907,
			"seed": 417801992,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554401026,
			"link": null,
			"locked": false,
			"fontSize": 15.099492256505298,
			"fontFamily": 4,
			"text": "[CLS]表示该特征用于分类模型，其对应的最后一个Transformer层输出\n被用来聚集整个序列的表征信息。对非分类模型，该符号可以省去。\n[SEP]表示分句符号，用于断开输入语料中的两个句子。",
			"rawText": "[CLS]表示该特征用于分类模型，其对应的最后一个Transformer层输出\n被用来聚集整个序列的表征信息。对非分类模型，该符号可以省去。\n[SEP]表示分句符号，用于断开输入语料中的两个句子。",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "[CLS]表示该特征用于分类模型，其对应的最后一个Transformer层输出\n被用来聚集整个序列的表征信息。对非分类模型，该符号可以省去。\n[SEP]表示分句符号，用于断开输入语料中的两个句子。",
			"lineHeight": 1.2,
			"baseline": 50
		},
		{
			"type": "text",
			"version": 100,
			"versionNonce": 1847790644,
			"isDeleted": false,
			"id": "r6yKRdug",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2352.5917032637635,
			"y": 2785.340981905875,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 75.44998168945312,
			"height": 18.119390707806357,
			"seed": 818254456,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": "https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/bert.html",
			"locked": false,
			"fontSize": 15.099492256505298,
			"fontFamily": 4,
			"text": "飞桨的介绍",
			"rawText": "飞桨的介绍",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "飞桨的介绍",
			"lineHeight": 1.2,
			"baseline": 14
		},
		{
			"type": "image",
			"version": 337,
			"versionNonce": 1116340492,
			"isDeleted": false,
			"id": "sPa9zmKUtyxIwhCyAtS5H",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1872.730757705037,
			"y": 3157.7315871773617,
			"strokeColor": "transparent",
			"backgroundColor": "transparent",
			"width": 532.8165370531549,
			"height": 87.7967517286352,
			"seed": 1393899384,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399117,
			"link": "",
			"locked": false,
			"status": "pending",
			"fileId": "464683fceb24c6e5f7ef4b35fe3a739f073e4139",
			"scale": [
				1,
				1
			]
		},
		{
			"type": "rectangle",
			"version": 908,
			"versionNonce": 64710068,
			"isDeleted": false,
			"id": "uSqI2R-_ENGHbw-NJpQ_9",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1809.7875409065646,
			"y": 3254.9672839455557,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 418,
			"height": 137,
			"seed": 712018952,
			"groupIds": [],
			"roundness": null,
			"boundElements": [
				{
					"type": "text",
					"id": "HzBfaLf8"
				}
			],
			"updated": 1686554399118,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 1069,
			"versionNonce": 1178468236,
			"isDeleted": false,
			"id": "HzBfaLf8",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1804.7875409065646,
			"y": 3260.0494164682336,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 405.0205078125,
			"height": 126.8357349546445,
			"seed": 1280328456,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399118,
			"link": null,
			"locked": false,
			"fontSize": 15.099492256505298,
			"fontFamily": 4,
			"text": "C为分类token（[CLS]）对应最后一个Transformer的输出，\nT1i则代表其他token对应最后一个Transformer的输出。对于\n一些token级别的任务（如，序列标注和问答任务），就把\nTi输入到额外的输出层中进行预测。对于一些句子级别的任务\nC（如自然语言推断和情感分类任务），就把C输入到额外的\n输出层中，这里也就解释了为什么要在每一个token序列前都\n要插入特定的分类token。",
			"rawText": "C为分类token（[CLS]）对应最后一个Transformer的输出，\n T1i则代表其他token对应最后一个Transformer的输出。对于一些token级别的任务（如，序列标注和问答任务），就把\n Ti输入到额外的输出层中进行预测。对于一些句子级别的任务C（如自然语言推断和情感分类任务），就把C输入到额外的输出层中，这里也就解释了为什么要在每一个token序列前都要插入特定的分类token。",
			"textAlign": "left",
			"verticalAlign": "middle",
			"containerId": "uSqI2R-_ENGHbw-NJpQ_9",
			"originalText": "C为分类token（[CLS]）对应最后一个Transformer的输出，\n T1i则代表其他token对应最后一个Transformer的输出。对于一些token级别的任务（如，序列标注和问答任务），就把\n Ti输入到额外的输出层中进行预测。对于一些句子级别的任务C（如自然语言推断和情感分类任务），就把C输入到额外的输出层中，这里也就解释了为什么要在每一个token序列前都要插入特定的分类token。",
			"lineHeight": 1.2,
			"baseline": 123
		},
		{
			"type": "rectangle",
			"version": 277,
			"versionNonce": 1145868084,
			"isDeleted": false,
			"id": "_86H0jXNtvjNWxWFwiPkG",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1371.8563173849263,
			"y": 3319.5685323140992,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 511,
			"height": 29,
			"seed": 665456760,
			"groupIds": [
				"7cCyIFMgIhM4BB7WQRwDH"
			],
			"roundness": null,
			"boundElements": [
				{
					"id": "jfTPFVWW",
					"type": "text"
				}
			],
			"updated": 1686554399118,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 318,
			"versionNonce": 1609444876,
			"isDeleted": false,
			"id": "jfTPFVWW",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1366.8332308370748,
			"y": 3325.0088369601963,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 500.9538269042969,
			"height": 18.119390707806357,
			"seed": 2029365880,
			"groupIds": [
				"7cCyIFMgIhM4BB7WQRwDH"
			],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399118,
			"link": null,
			"locked": false,
			"fontSize": 15.099492256505298,
			"fontFamily": 4,
			"text": "Next Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文",
			"rawText": "Next Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "_86H0jXNtvjNWxWFwiPkG",
			"originalText": "Next Sentence Prediction（NSP）的任务是判断句子B是否是句子A的下文",
			"lineHeight": 1.2,
			"baseline": 14
		},
		{
			"type": "rectangle",
			"version": 324,
			"versionNonce": 2145159348,
			"isDeleted": false,
			"id": "YzuD5pBQrNHSNKsjYnDb9",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1370.8881724764283,
			"y": 3257.891802085037,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 519,
			"height": 47,
			"seed": 717730312,
			"groupIds": [
				"7cCyIFMgIhM4BB7WQRwDH"
			],
			"roundness": null,
			"boundElements": [
				{
					"id": "loX2AJUq",
					"type": "text"
				}
			],
			"updated": 1686554399118,
			"link": null,
			"locked": false
		},
		{
			"type": "text",
			"version": 415,
			"versionNonce": 1620348044,
			"isDeleted": false,
			"id": "loX2AJUq",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -1365.4092296053345,
			"y": 3263.2724113772306,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 508.0421142578125,
			"height": 36.23878141561271,
			"seed": 523948296,
			"groupIds": [
				"7cCyIFMgIhM4BB7WQRwDH"
			],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399118,
			"link": null,
			"locked": false,
			"fontSize": 15.099492256505298,
			"fontFamily": 4,
			"text": "MLM是指在训练的时候随即从输入语料上mask掉一些单词，然后通过的上下\n文预测该单词，该任务非常像我们在中学时期经常做的完形填空。",
			"rawText": "MLM是指在训练的时候随即从输入语料上mask掉一些单词，然后通过的上下\n文预测该单词，该任务非常像我们在中学时期经常做的完形填空。",
			"textAlign": "center",
			"verticalAlign": "middle",
			"containerId": "YzuD5pBQrNHSNKsjYnDb9",
			"originalText": "MLM是指在训练的时候随即从输入语料上mask掉一些单词，然后通过的上下\n文预测该单词，该任务非常像我们在中学时期经常做的完形填空。",
			"lineHeight": 1.2,
			"baseline": 32
		},
		{
			"type": "text",
			"version": 162,
			"versionNonce": 171004468,
			"isDeleted": false,
			"id": "nf5kHW9R",
			"fillStyle": "hachure",
			"strokeWidth": 1,
			"strokeStyle": "solid",
			"roughness": 1,
			"opacity": 100,
			"angle": 0,
			"x": -2274.0583286065157,
			"y": -234.8075867856649,
			"strokeColor": "#1e1e1e",
			"backgroundColor": "transparent",
			"width": 176,
			"height": 52.8,
			"seed": 99652,
			"groupIds": [],
			"roundness": null,
			"boundElements": [],
			"updated": 1686554399118,
			"link": "[[展示]]",
			"locked": false,
			"fontSize": 44,
			"fontFamily": 4,
			"text": "展示讲稿",
			"rawText": "展示讲稿",
			"textAlign": "left",
			"verticalAlign": "top",
			"containerId": null,
			"originalText": "展示讲稿",
			"lineHeight": 1.2,
			"baseline": 43
		}
	],
	"appState": {
		"theme": "light",
		"viewBackgroundColor": "#fffce8",
		"currentItemStrokeColor": "#1e1e1e",
		"currentItemBackgroundColor": "transparent",
		"currentItemFillStyle": "hachure",
		"currentItemStrokeWidth": 1,
		"currentItemStrokeStyle": "solid",
		"currentItemRoughness": 1,
		"currentItemOpacity": 100,
		"currentItemFontFamily": 4,
		"currentItemFontSize": 44,
		"currentItemTextAlign": "left",
		"currentItemStartArrowhead": null,
		"currentItemEndArrowhead": "arrow",
		"scrollX": 1928.860123420962,
		"scrollY": -944.0264464286988,
		"zoom": {
			"value": 0.7710982109473627
		},
		"currentItemRoundness": "sharp",
		"gridSize": null,
		"currentStrokeOptions": null,
		"previousGridSize": null
	},
	"files": {}
}
```
%%