### 神经网络与深度学习基础

- [闲聊：神经网络与深度学习](https://spaces.ac.cn/archives/3331)
- [从Boosting学习到神经网络：看山是山？](https://spaces.ac.cn/archives/3873)

---

### 词向量与Embedding技术

- [词向量与Embedding究竟是怎么回事？](https://spaces.ac.cn/archives/4122)
- [【不可思议的Word2Vec】 1.数学原理](https://spaces.ac.cn/archives/4299)
- [【不可思议的Word2Vec】 2.训练好的模型](https://spaces.ac.cn/archives/4304)
- [【不可思议的Word2Vec】 3.提取关键词](https://spaces.ac.cn/archives/4316)
- [【不可思议的Word2Vec】 4.不一样的“相似”](https://spaces.ac.cn/archives/4368)

---

### 奇异值分解（SVD）与应用

- [SVD分解(一)：自编码器与人工智能](https://spaces.ac.cn/archives/4208)
- [SVD分解(二)：为什么SVD意味着聚类？](https://spaces.ac.cn/archives/4216)
- [SVD分解(三)：连Word2Vec都只不过是个SVD？](https://spaces.ac.cn/archives/4233)

---

### **优化算法与梯度下降**

- [梯度下降和EM算法：系出同源，一脉相承](https://spaces.ac.cn/archives/4277)
- [从动力学角度看优化算法（一）：从SGD到动量加速](https://spaces.ac.cn/archives/5655)
- [从动力学角度看优化算法（二）：自适应学习率算法](https://spaces.ac.cn/archives/6234)
- [从动力学角度看优化算法（三）：一个更整体的视角](https://spaces.ac.cn/archives/6261)
- [从动力学角度看优化算法（四）：GAN的第三个阶段](https://spaces.ac.cn/archives/6583)
- [从动力学角度看优化算法（五）：为什么学习率不宜过小？](https://spaces.ac.cn/archives/7787)
- [从动力学角度看优化算法（六）：为什么SimSiam不退化？](https://spaces.ac.cn/archives/7980)
- [从动力学角度看优化算法（七）：SGD ≈ SVM？](https://spaces.ac.cn/archives/8009)

---

### 自编码器与变分自编码器（VAE）

- [变分自编码器（一）：原来是这么一回事](https://spaces.ac.cn/archives/5253)
- [变分自编码器（三）：这样做为什么能成？](https://spaces.ac.cn/archives/5383)
- [变分自编码器（四）：一步到位的聚类方案](https://spaces.ac.cn/archives/5887)
- [变分自编码器（五）：VAE + BN = 更好的VAE](https://spaces.ac.cn/archives/7381)
- [变分自编码器（六）：从几何视角来理解VAE的尝试](https://spaces.ac.cn/archives/7725)
- [变分自编码器（七）：球面上的VAE（vMF-VAE）](https://spaces.ac.cn/archives/8404)
- [变分自编码器（八）：估计样本概率密度](https://spaces.ac.cn/archives/8791)

---

### 生成模型与生成对抗网络（GAN）

- [互怼的艺术：从零直达WGAN-GP](https://spaces.ac.cn/archives/4439)
- [用变分推断统一理解生成模型（VAE、GAN、AAE、ALI）](https://spaces.ac.cn/archives/5716)
- [f-GAN简介：GAN模型的生产车间](https://spaces.ac.cn/archives/6016)
- [能量视角下的GAN模型（一）：GAN＝“挖坑”＋“跳坑”](https://spaces.ac.cn/archives/6316)
- [能量视角下的GAN模型（二）：GAN＝“分析”＋“采样”](https://spaces.ac.cn/archives/6331)
- [能量视角下的GAN模型（三）：生成模型=能量模型](https://spaces.ac.cn/archives/6612)
- [巧断梯度：单个loss实现GAN模型](https://spaces.ac.cn/archives/6387)
- [非对抗式生成模型GLANN的简单介绍](https://spaces.ac.cn/archives/6394)
- [O-GAN：简单修改，让GAN的判别器变成一个编码器！](https://spaces.ac.cn/archives/6409)
- [从DCGAN到SELF-MOD：GAN的模型架构发展一览](https://spaces.ac.cn/archives/6549)
- [RSGAN：对抗模型中的“图灵测试”思想](https://spaces.ac.cn/archives/6110)
- [WGAN-div：一个默默无闻的WGAN填坑者](https://spaces.ac.cn/archives/6139)
- [不用L约束又不会梯度消失的GAN，了解一下？](https://spaces.ac.cn/archives/6163)
- [BiGAN-QP：简单清晰的编码&生成模型](https://spaces.ac.cn/archives/6214)
- [对抗训练浅谈：意义、方法和思考（附Keras实现）](https://spaces.ac.cn/archives/7234)
- [WGAN的成功，可能跟Wasserstein距离没啥关系](https://spaces.ac.cn/archives/8244)

---

### **Transformer模型**

- [《Attention is All You Need》浅读（简介+代码）](https://spaces.ac.cn/archives/4765)
- [从语言模型到Seq2Seq：Transformer如戏，全靠Mask](https://spaces.ac.cn/archives/6933)
- [ON-LSTM：用有序神经元表达层次结构](https://spaces.ac.cn/archives/6621)
- [VQ-VAE的简明介绍：量子化自编码器](https://spaces.ac.cn/archives/6760)
- [突破瓶颈，打造更强大的Transformer](https://spaces.ac.cn/archives/7325)
- [Transformer升级之路：2、博采众长的旋转式位置编码](https://spaces.ac.cn/archives/8265)
- [Transformer升级之路：3、从Performer到线性Attention](https://spaces.ac.cn/archives/8338)
- [Transformer升级之路：4、二维位置的旋转式位置编码](https://spaces.ac.cn/archives/8397)
- [Transformer升级之路：5、作为无限维的线性Attention](https://spaces.ac.cn/archives/8601)
- [Transformer升级之路：6、旋转位置编码的完备性分析](https://spaces.ac.cn/archives/9403)
- [Transformer升级之路：7、长度外推性与局部注意力](https://spaces.ac.cn/archives/9431)
- [Transformer升级之路：8、长度外推性与位置鲁棒性](https://spaces.ac.cn/archives/9444)
- [Transformer升级之路：9、一种全局长度外推的新思路](https://spaces.ac.cn/archives/9603)
- [Transformer升级之路：10、RoPE是一种β进制编码](https://spaces.ac.cn/archives/9675)
- [Transformer升级之路：11、将β进制位置进行到底](https://spaces.ac.cn/archives/9706)
- [Transformer升级之路：12、无限外推的ReRoPE？](https://spaces.ac.cn/archives/9708)
- [Transformer升级之路：13、逆用Leaky ReRoPE](https://spaces.ac.cn/archives/9728)
- [Transformer升级之路：14、当HWFA遇见ReRoPE](https://spaces.ac.cn/archives/9731)
- [Transformer升级之路：15、Key归一化助力长度外推](https://spaces.ac.cn/archives/9859)
- [Transformer升级之路：16、“复盘”长度外推技术](https://spaces.ac.cn/archives/9948)
- [Transformer升级之路：17、多模态位置编码的简单思考](https://spaces.ac.cn/archives/10040)
- [Transformer升级之路：18、RoPE的底数选择原则](https://spaces.ac.cn/archives/10122)
- [训练1000层的Transformer究竟有什么困难？](https://spaces.ac.cn/archives/8978)
- [浅谈Transformer的初始化、参数化与标准化](https://spaces.ac.cn/archives/8620)
- [为什么现在的LLM都是Decoder-only的架构？](https://spaces.ac.cn/archives/9529)
- [《为什么现在的LLM都是Decoder-only的架构？》FAQ](https

://[http://spaces.ac.cn/archives/9547](https://link.zhihu.com/?target=http%3A//spaces.ac.cn/archives/9547)) - [线性Transformer应该不是你要等的那个模型](https://spaces.ac.cn/archives/8610)

---

### **生成扩散模型**

- [生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼](https://spaces.ac.cn/archives/9119)
- [生成扩散模型漫谈（二）：DDPM = 自回归式VAE](https://spaces.ac.cn/archives/9152)
- [生成扩散模型漫谈（三）：DDPM = 贝叶斯 + 去噪](https://spaces.ac.cn/archives/9164)
- [生成扩散模型漫谈（四）：DDIM = 高观点DDPM](https://spaces.ac.cn/archives/9181)
- [生成扩散模型漫谈（五）：一般框架之SDE篇](https://spaces.ac.cn/archives/9209)
- [生成扩散模型漫谈（六）：一般框架之ODE篇](https://spaces.ac.cn/archives/9228)
- [生成扩散模型漫谈（七）：最优扩散方差估计（上）](https://spaces.ac.cn/archives/9245)
- [生成扩散模型漫谈（八）：最优扩散方差估计（下）](https://spaces.ac.cn/archives/9246)
- [生成扩散模型漫谈（九）：条件控制生成结果](https://spaces.ac.cn/archives/9257)
- [生成扩散模型漫谈（十）：统一扩散模型（理论篇）](https://spaces.ac.cn/archives/9262)
- [生成扩散模型漫谈（十一）：统一扩散模型（应用篇）](https://spaces.ac.cn/archives/9271)
- [生成扩散模型漫谈（十二）：“硬刚”扩散ODE](https://spaces.ac.cn/archives/9280)
- [生成扩散模型漫谈（十三）：从万有引力到扩散模型](https://spaces.ac.cn/archives/9305)
- [生成扩散模型漫谈（十四）：构建ODE的一般步骤（上）](https://spaces.ac.cn/archives/9370)
- [生成扩散模型漫谈（十五）：构建ODE的一般步骤（中）](https://spaces.ac.cn/archives/9379)
- [生成扩散模型漫谈（十六）：W距离 ≤ 得分匹配](https://spaces.ac.cn/archives/9467)
- [生成扩散模型漫谈（十七）：构建ODE的一般步骤（下）](https://spaces.ac.cn/archives/9497)
- [生成扩散模型漫谈（十八）：得分匹配 = 条件得分匹配](https://spaces.ac.cn/archives/9509)
- [生成扩散模型漫谈（十九）：作为扩散ODE的GAN](https://spaces.ac.cn/archives/9662)
- [生成扩散模型漫谈（二十）：从ReFlow到WGAN-GP](https://spaces.ac.cn/archives/9668)
- [生成扩散模型漫谈（二十一）：中值定理加速ODE采样](https://spaces.ac.cn/archives/9881)
- [生成扩散模型漫谈（二十二）：信噪比与大图生成（上）](https://spaces.ac.cn/archives/10047)
- [生成扩散模型漫谈（二十三）：信噪比与大图生成（下）](https://spaces.ac.cn/archives/10055)
- [生成扩散模型漫谈（二十四）：少走捷径，更快到达](https://spaces.ac.cn/archives/10077)

---

### **随机场与互信息**

- [果壳中的条件随机场(CRF In A Nutshell)](https://spaces.ac.cn/archives/4695)
- [深度学习的互信息：无监督提取特征](https://spaces.ac.cn/archives/6024)
- [深度学习中的Lipschitz约束：泛化与生成模型](https://spaces.ac.cn/archives/6051)
- [变分自编码器 = 最小化先验分布 + 最大化互信息](https://spaces.ac.cn/archives/6088)
- [从变分编码、信息瓶颈到正态分布：论遗忘的重要性](https://spaces.ac.cn/archives/6181)

---

### **正则化与损失函数**

- [从loss的硬截断、软化到focal loss](https://spaces.ac.cn/archives/4733)
- [再谈类别不平衡问题：调节权重与魔改Loss的对比联系](https://spaces.ac.cn/archives/7708)
- [L2正则没有想象那么好？可能是“权重尺度偏移”惹的祸](https://spaces.ac.cn/archives/7681)
- [从SamplePairing到mixup：神奇的正则项](https://spaces.ac.cn/archives/5693)
- [从梯度最大化看Attention的Scale操作](https://spaces.ac.cn/archives/9812)

---

### **多任务学习**

- [多任务学习漫谈（一）：以损失之名](https://spaces.ac.cn/archives/8870)
- [多任务学习漫谈（二）：行梯度之事](https://spaces.ac.cn/archives/8896)
- [多任务学习漫谈（三）：分主次之序](https://spaces.ac.cn/archives/8907)

---

### **注意力机制**

- [听说Attention与Softmax更配哦～](https://spaces.ac.cn/archives/9019)
- [Dropout视角下的MLM和MAE：一些新的启发](https://spaces.ac.cn/archives/8770)
- [线性Attention的探索：Attention必须有个Softmax吗？](https://spaces.ac.cn/archives/7546)
- [Google新作Synthesizer：我们还不够了解自注意力](https://spaces.ac.cn/archives/7430)
- [注意力机制真的可以“集中注意力”吗？](https://spaces.ac.cn/archives/9889)

---

### **随机过程与贝叶斯**

- [最小熵原理（一）：无监督学习的原理](https://spaces.ac.cn/archives/5448)
- [最小熵原理（二）：“当机立断”之词库构建](https://spaces.ac.cn/archives/5476)
- [最小熵原理（四）：“物以类聚”之从图书馆到词向量](https://spaces.ac.cn/archives/6191)
- [最小熵原理（五）：“层层递进”之社区发现与聚类](https://spaces.ac.cn/archives/7006)
- [最小熵原理（六）：词向量的维度应该怎么选择？](https://spaces.ac.cn/archives/7695)

---

### **其他**

- [【备忘】谈谈dropout](https://spaces.ac.cn/archives/4521)
- [BN究竟起了什么作用？一个闭门造车的分析](https://spaces.ac.cn/archives/6992)
- [基于Amos优化器思想推导出来的一些“炼丹策略”](https://spaces.ac.cn/archives/9344)
- [用热传导方程来指导自监督学习](https://spaces.ac.cn/archives/9359)
- [配置不同的学习率，LoRA还能再涨一点？](https://spaces.ac.cn/archives/10001)
- [为什么需要残差？一个来自DeepNet的视角](https://spaces.ac.cn/archives/8994)
- [为什么Pre Norm的效果不如Post Norm？](https://spaces.ac.cn/archives/9009)
- [缓解交叉熵过度自信的一个简明方案](https://spaces.ac.cn/archives/9526)
- [简单得令人尴尬的FSQ：“四舍五入”超越了VQ-VAE](https://spaces.ac.cn/archives/9826)
- [“维度灾难”之Hubness现象浅析](https://spaces.ac.cn/archives/9147)
- [隐藏在动量中的梯度累积：少更新几步，效果反而更好？](https://spaces.ac.cn/archives/8634)
- [幂等生成网络IGN：试图将判别和生成合二为一的GAN](https://spaces.ac.cn/archives/9969)
- [以蒸馏的名义：“从去噪自编码器到生成模型”重现江湖](https://spaces.ac.cn/archives/10085)

---

### **OCR技术**

- [OCR技术浅探：1. 全文简述](https://spaces.ac.cn/archives/3774)
- [OCR技术浅探：2. 背景与假设](https://spaces.ac.cn/archives/3781)
- [OCR技术浅探：3. 特征提取(1)](https://spaces.ac.cn/archives/3785)
- [OCR技术浅探：3. 特征提取(2)](https://spaces.ac.cn/archives/3802)
- [OCR技术浅探：4. 文字定位](https://spaces.ac.cn/archives/3818)  
    
- [OCR技术浅探：5. 文本切割](https://spaces.ac.cn/archives/3823)  
    
- [OCR技术浅探：6. 光学识别](https://spaces.ac.cn/archives/3831)
- [OCR技术浅探：7. 语言模型](https://spaces.ac.cn/archives/3842)
- [OCR技术浅探：8. 综合评估](https://spaces.ac.cn/archives/3854)
- [OCR技术浅探：9. 代码共享(完)](https://spaces.ac.cn/archives/3856)